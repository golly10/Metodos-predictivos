{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk9mtJJOegTX"
      },
      "source": [
        "# Entrenamiento de clasificadores avanzados en *scikit-learn*\n",
        "En este notebook aprenderás a entrenar clasificadores avanzados en Python gracias a la librería *scikit-learn*. Además, también se compararán en distintos escenarios a los clasificadores más convencionales, tanto en rendimiento predictivo como en complejidad. Así, se tratará de discernir qué método (o métodos) es mejor o más recomendable en cada caso.\n",
        "\n",
        "Es recomendable tener en todo momento disponible la [Guía de usuario de *scikit-learn*](https://scikit-learn.org/stable/user_guide.html), o la documentación de la [API](https://scikit-learn.org/stable/modules/classes.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3dHp9YUe8RD"
      },
      "source": [
        "## 1.   Carga de datos\n",
        "\n",
        "A lo largo del notebook, en este caso, vamos a utilizar 4 conjuntos de datos distintos que cargaremos en esta primer celda. A su vez, cuando sea necesario, realizaremos particiones de entrenamiento y test. Algunos conjuntos de datos se utilizan ya en las distintas celdas de código; para el resto de datasets se aconseja al estudiante que modifique el código necesario para poder realizar los mismos procesos con distintos conjuntos de datos.\n",
        "\n",
        "En primer lugar, como en casos anteriores, el conjunto de datos de [*Breast Cancer*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). Este conjunto de datos de juguete contiene 569 patrones en total, cada uno descrito por 30 atributos, y con dos clases de salida.\n",
        "\n",
        "En segundo lugar, vamos a cargar un conjunto de datos de otro problema binario, pero siendo en este caso un problema real: el dataset [Labeled Faces in the Wild (pairs)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_pairs.html#sklearn.datasets.fetch_lfw_pairs). Este conjunto de datos contiene información de imágenes de caras de personas famosas. Uno de los problemas para este conjunto de datos sería [identificar el nombre de la persona dada la imagen](https://scikit-learn.org/stable/datasets/real_world.html#labeled-faces-in-the-wild-dataset). Sin embargo, en el conjunto [*pairs*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_pairs.html#sklearn.datasets.fetch_lfw_pairs), que es el que utilizaremos en este notebook, cada instancia contiene información de dos caras, y la clase es binaria, indicando si se trata o no de la misma persona. El conjunto de datos incluye más de 13000 patrones descritos cada uno con más de 5800 atributos de entrada (correspondientes a dos imágenes de 62*47 pixels).\n",
        "\n",
        "También utilizaremos otro problema real de clasificación multi-clase. El conjunto de datos [Olivetti Faces](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces) contiene también imágenes de 40 personas en distintos escenarios de luz, expresión facial, etc. La tarea se basa en identificar la identidad de la persona dada su imagen. Este conjunto de datos contiene 400 patrones descritos por 4096 atributos de entrada (imágenes de 92*112 pixels), y un total de 40 clases distintas.\n",
        "\n",
        "Por último, utilizaremos el dataset [MNIST](https://www.openml.org/d/554) para reconocimiento de dígitos manuscritos. Este conjunto se describió en la tercera lección del curso, pero puede encontrar más información [aquí](http://yann.lecun.com/exdb/mnist/). El conjunto de datos contiene 60000 patrones de entrenamiento y 10000 de test, cada uno de ellos descrito por 784 atributos de entrada (imágenes de 28*28 pixels), y un total de 10 clases (dígitos del 0 al 9).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Zb8ISLewbTH"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_pairs, fetch_olivetti_faces, load_breast_cancer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Lo utilizaremos para escalar todas las variables al mismo rango\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Cargamos el conjunto de datos Cancer y dividimos en train-test\n",
        "cancer_X, cancer_y = load_breast_cancer(return_X_y=True)\n",
        "cancer_X_train, cancer_X_test, cancer_y_train, cancer_y_test = train_test_split(cancer_X, cancer_y, test_size=0.2, random_state=0)\n",
        "\n",
        "cancer_X_train = scaler.fit_transform(cancer_X_train)\n",
        "cancer_X_test = scaler.transform(cancer_X_test)\n",
        "\n",
        "# Cargamos el conjunto de LFW, que se proporciona directamente en conjuntos de entranamiento y test\n",
        "lfw_train = fetch_lfw_pairs(subset='train')\n",
        "lfw_X_train = lfw_train.data\n",
        "lfw_y_train = lfw_train.target\n",
        "lfw_test = fetch_lfw_pairs(subset='test')\n",
        "lfw_X_test = lfw_test.data\n",
        "lfw_y_test = lfw_test.target\n",
        "\n",
        "lfw_X_train = scaler.fit_transform(lfw_X_train)\n",
        "lfw_X_test = scaler.transform(lfw_X_test)\n",
        "\n",
        "# Cargamos el conjunto de olivetti faces y dividimos en train-test\n",
        "olivetti_X, olivetti_y = fetch_olivetti_faces(return_X_y=True)\n",
        "olivetti_X_train, olivetti_X_test, olivetti_y_train, olivetti_y_test = train_test_split(olivetti_X, olivetti_y, test_size=0.2, random_state=0)\n",
        "\n",
        "olivetti_X_train = scaler.fit_transform(olivetti_X_train)\n",
        "olivetti_X_test = scaler.transform(olivetti_X_test)\n",
        "\n",
        "# Cargamos y preprocesamos dataset MNIST\n",
        "mnist_X, mnist_y = fetch_openml(\"mnist_784\", return_X_y=True, as_frame=False)\n",
        "# Originalmente serian 60000 instancias de entrenamiento y 10000 de test, pero para hacer el proceso más rápido, dejaremos 6000, y 1000 de test\n",
        "mnist_X_train, mnist_X_test, mnist_y_train, mnist_y_test = train_test_split(mnist_X, mnist_y, train_size=5000, test_size=1000, random_state=0)\n",
        "\n",
        "mnist_X_train = scaler.fit_transform(mnist_X_train)\n",
        "mnist_X_test = scaler.transform(mnist_X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Dhg3C-pAdT"
      },
      "source": [
        "## 2.   Entrenamiento de modelos avanzados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QswUxBaFtrbi"
      },
      "source": [
        "### 2.1.  SVM\n",
        "\n",
        "En primer lugar, vamos a entrenar modelos de SVM. Para ello, vamos a realizar un proceso de búsqueda de parámetros en dos fases: en primer lugar, entrenaremos el [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) con distintos kernels; y en segundo lugar, para el kernel que mejor funcionamiento tuviese, optimizaremos su valor de C.\n",
        "\n",
        "En primer lugar, ejecutamos SVM con kernels lineal, polinómico (por defecto, grado 3), RBF (funciones gausianas), y sigmoide.\n",
        "\n",
        "En segundo lugar, utilizamos valores de C desde 1e-3 hasta 1e3; mayores valores de C hacen preferible un margen menor siempre que los datos de entrenamiento se clasifiquen mejor; valores menores de C obtienen márgenes mayores pese a que la frontera sea más simple y suponga una pérdida en rendimiento predictivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jkukgNcPc9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "linear\n",
            "poly\n",
            "rbf\n",
            "sigmoid\n",
            "\n",
            "Resultados\n",
            "['linear', 'poly', 'rbf', 'sigmoid']\n",
            "[7.696641249999999, 4.837471208000004, 4.754057249999988, 3.936020499999998]\n",
            "[0.525, 0.563, 0.626, 0.483]\n",
            "[0.5329400196656834, 0.6261762189905903, 0.6375968992248061, 0.4916420845624386]\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "# Buscar kernel con mejor rendimiento\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "times = [-1] * len(kernels)\n",
        "acc = [-1] * len(kernels)\n",
        "f1 = [-1] * len(kernels)\n",
        "\n",
        "for i in range(len(kernels)):\n",
        "  print(kernels[i])\n",
        "  tic = timeit.default_timer()\n",
        "  svm_model = svm.SVC(kernel=kernels[i]).fit(lfw_X_train, lfw_y_train)\n",
        "  toc = timeit.default_timer()\n",
        "  times[i] = toc - tic\n",
        "\n",
        "  svm_pred = svm_model.predict(lfw_X_test)\n",
        "\n",
        "  acc[i] = metrics.accuracy_score(lfw_y_test, svm_pred)\n",
        "  f1[i] = metrics.f1_score(lfw_y_test, svm_pred)\n",
        "\n",
        "print('\\nResultados')\n",
        "print(kernels)\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HULAfPRrgxks"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.001\n",
            "0.01\n",
            "0.1\n",
            "1.0\n",
            "10.0\n",
            "100.0\n",
            "1000.0\n",
            "\n",
            "Resultados\n",
            "[0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
            "[5.109994084000007, 5.385730707999997, 5.1610819590000006, 4.844310874999991, 5.31380320800001, 5.19716112499998, 5.6544541249999725]\n",
            "[0.587, 0.587, 0.601, 0.626, 0.617, 0.617, 0.617]\n",
            "[0.5693430656934307, 0.5693430656934307, 0.6159769008662176, 0.6375968992248061, 0.6277939747327502, 0.6277939747327502, 0.6277939747327502]\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "# Buscar mejor valor de C para un kernel concreto\n",
        "Cs = [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
        "\n",
        "times = [-1] * len(Cs)\n",
        "acc = [-1] * len(Cs)\n",
        "f1 = [-1] * len(Cs)\n",
        "\n",
        "for i in range(len(Cs)):\n",
        "  print(Cs[i])\n",
        "  tic = timeit.default_timer()\n",
        "  svm_model = svm.SVC(kernel='rbf', C=Cs[i]).fit(lfw_X_train, lfw_y_train)\n",
        "  toc = timeit.default_timer()\n",
        "  times[i] = toc - tic\n",
        "\n",
        "  svm_pred = svm_model.predict(lfw_X_test)\n",
        "\n",
        "  acc[i] = metrics.accuracy_score(lfw_y_test, svm_pred)\n",
        "  f1[i] = metrics.f1_score(lfw_y_test, svm_pred)\n",
        "\n",
        "print('\\nResultados')\n",
        "print(Cs)\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_r84ff0vic1"
      },
      "source": [
        "### 2.2. Random Forest\n",
        "\n",
        "Ahora, vamos a entrenar distintos modelos de [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier). En este caso, vamos a variar el número de árboles del ensemble y la profundidad máxima de cada uno de ellos, siguiendo un método de rejilla o *grid* (es decir, probar todas las combinaciones de ambos parámetros, no como en el caso del SVM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eKaWjdcXjg07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "  0\n",
            "  1\n",
            "  2\n",
            "1\n",
            "  0\n",
            "  1\n",
            "  2\n",
            "2\n",
            "  0\n",
            "  1\n",
            "  2\n",
            "3\n",
            "  0\n",
            "  1\n",
            "  2\n",
            "4\n",
            "  0\n",
            "  1\n",
            "  2\n",
            "\n",
            "Resultados\n",
            "[[0.7331963750000057, 0.19958862500001828, 0.3305500410000093], [3.699524083, 0.9892573329999834, 1.5993371669999874], [7.40552466699998, 1.9613925829999914, 3.145623124999986], [36.99846208300002, 9.83803341700002, 16.049145208], [73.363579958, 19.446321875000024, 31.734129333]]\n",
            "[[0.577, 0.549, 0.551], [0.559, 0.56, 0.567], [0.602, 0.576, 0.59], [0.61, 0.591, 0.592], [0.607, 0.592, 0.589]]\n",
            "[[0.5187713310580205, 0.5547877591312932, 0.5619512195121951], [0.5620655412115194, 0.5537525354969575, 0.568295114656032], [0.5930470347648262, 0.5867446393762183, 0.5964566929133858], [0.6161417322834645, 0.5930348258706467, 0.6023391812865496], [0.6158357771260997, 0.6015625, 0.6044273339749759]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Número de árboles en el ensemble\n",
        "n = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Profundidad máxima de cada árbol\n",
        "max_depth_trees = [None, 3, 5]\n",
        "\n",
        "times = [None] * len(n)\n",
        "acc = [None] * len(n)\n",
        "f1 = [None] * len(n)\n",
        "\n",
        "\n",
        "for i in range(len(n)):\n",
        "  print(i)\n",
        "  times[i] = [-1] * len(max_depth_trees)\n",
        "  acc[i] = [-1] * len(max_depth_trees)\n",
        "  f1[i] = [-1] * len(max_depth_trees)\n",
        "  for j in range(len(max_depth_trees)):\n",
        "    print('  ' + str(j))\n",
        "    tic = timeit.default_timer()\n",
        "    rf = RandomForestClassifier(n_estimators=n[i], max_depth=max_depth_trees[j]).fit(lfw_X_train, lfw_y_train)\n",
        "    toc = timeit.default_timer()\n",
        "    times[i][j] = toc - tic\n",
        "\n",
        "    rf_pred = rf.predict(lfw_X_test)\n",
        "\n",
        "    acc[i][j] = metrics.accuracy_score(lfw_y_test, rf_pred)\n",
        "    f1[i][j] = metrics.f1_score(lfw_y_test, rf_pred)\n",
        "\n",
        "print('\\nResultados')\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aFXudwowEly"
      },
      "source": [
        "### 2.3. Redes neuronales \n",
        "\n",
        "Como último clasificador avanzado, entrenaremos un modelo de Red Neuronal Artificial, llamado [MLP - *Multi-Layer Perceptron*](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) en *scikit-learn*).\n",
        "\n",
        "En este caso, vamos a variar el número de nodos en capa oculta y el número de capas ocultas, entre algunos valores arbitrarios escogidos. Hay muchos otros parámetros que podían optimizarse, pero vamos a centrarnos en este."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TUwTIZA8oRM0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'math' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/0n/nxywfqj174j8jrc5w4btcbb40000gn/T/ipykernel_1645/3523321717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distintas configuraciones de capas ocultas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Cada tupla indica el número de neuronas en cada una de las capas ocultas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "n_attr = 5828\n",
        "\n",
        "# Distintas configuraciones de capas ocultas\n",
        "# Cada tupla indica el número de neuronas en cada una de las capas ocultas\n",
        "hidden = [(10,), (50,), (50, 10,), (100,), (100, 50,), (round(math.sqrt(n_attr)),), (round(math.sqrt(n_attr)), round(math.log2(n_attr)))]\n",
        "times = [None] * len(hidden)\n",
        "acc = [None] * len(hidden)\n",
        "f1 = [None] * len(hidden)\n",
        "\n",
        "for i in range(len(hidden)):\n",
        "  print(hidden[i])\n",
        "  tic = timeit.default_timer()\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=hidden[i], random_state=0).fit(lfw_X_train, lfw_y_train)\n",
        "  toc = timeit.default_timer()\n",
        "\n",
        "  times[i] = toc - tic\n",
        "\n",
        "  mlp_pred = mlp.predict(lfw_X_test)\n",
        "\n",
        "  acc[i] = metrics.accuracy_score(lfw_y_test, mlp_pred)\n",
        "  f1[i] = metrics.f1_score(lfw_y_test, mlp_pred)\n",
        "\n",
        "print('\\nResultados')\n",
        "print(hidden)\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmLL_kxVpMnv"
      },
      "source": [
        "## 3. Entrenamiento de modelos clásicos y comparación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e38AdBUdwhiU"
      },
      "source": [
        "Posteriormente, vamos a entrenar varios modelos clásicos, como regresión logística, kNN, árbol de decisión, y un Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6WUdzwYse7B"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Entrenar clasificadores clásicos con conjunto de datos de lfw\n",
        "tic = timeit.default_timer()\n",
        "lr = LogisticRegression(random_state=0).fit(lfw_X_train, lfw_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('LR train time: ' + str(toc - tic))\n",
        "\n",
        "lr_pred = lr.predict(lfw_X_test)\n",
        "\n",
        "print('LR acc: ' + str(metrics.accuracy_score(lfw_y_test, lr_pred)))\n",
        "print('LR F1: ' + str(metrics.f1_score(lfw_y_test, lr_pred)))\n",
        "print('---')\n",
        "\n",
        "tic = timeit.default_timer()\n",
        "tree = tree.DecisionTreeClassifier().fit(lfw_X_train, lfw_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('Tree train time: ' + str(toc - tic))\n",
        "\n",
        "tree_pred = tree.predict(lfw_X_test)\n",
        "\n",
        "print('Tree acc: ' + str(metrics.accuracy_score(lfw_y_test, tree_pred)))\n",
        "print('Tree F1: ' + str(metrics.f1_score(lfw_y_test, tree_pred)))\n",
        "print('---')\n",
        "\n",
        "tic = timeit.default_timer()\n",
        "gnb = GaussianNB().fit(lfw_X_train, lfw_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('GNB train time: ' + str(toc - tic))\n",
        "\n",
        "gnb_pred = gnb.predict(lfw_X_test)\n",
        "\n",
        "print('GNB acc: ' + str(metrics.accuracy_score(lfw_y_test, gnb_pred)))\n",
        "print('GNB F1: ' + str(metrics.f1_score(lfw_y_test, gnb_pred)))\n",
        "print('---')\n",
        "\n",
        "# Tener en cuenta que con ciertos conjuntos de datos puede ser MUY costoso obtener las predicciones en test\n",
        "tic = timeit.default_timer()\n",
        "knn = KNeighborsClassifier(n_neighbors=3).fit(lfw_X_train, lfw_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('kNN train time: ' + str(toc - tic))\n",
        "\n",
        "tic = timeit.default_timer()\n",
        "knn_pred = knn.predict(lfw_X_test)\n",
        "toc = timeit.default_timer()\n",
        "print('kNN testing time: ' + str(toc - tic))\n",
        "\n",
        "print('kNN acc: ' + str(metrics.accuracy_score(lfw_y_test, knn_pred)))\n",
        "print('kNN F1: ' + str(metrics.f1_score(lfw_y_test, knn_pred)))\n",
        "print('---')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aTElGWKw0Ok"
      },
      "source": [
        "Compare los resultados de los distintos métodos (en los métodos avanzados, tenga en cuenta el resultado del mejor modelo obtenido).\n",
        "\n",
        "\n",
        "*   ¿Qué métodos obtienen los mejores resultados? ¿Los avanzados o los clásicos?\n",
        "*   En caso de que un método avanzado sea mejor que los clásicos, compare los tiempos de entrenamiento. Si el tiempo del método avanzado es mayor que el del clásico, ¿merece la pena ese incremento en tiempo para el incremento en rendimiento predictivo?\n",
        "*   Fijese en el rendimiento predictivo del árbol de decisión y de Random Forest (ensemble de árboles de decisión). ¿Mejora Random Forest los resultados del árbol de decisión? Es decir, consigue un método de ensemble superar el rendimiento de su clasificador base?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmTxWiXpWsc"
      },
      "source": [
        "## 4. Análisis en otro conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCeBFgavyb_H"
      },
      "source": [
        "Vamos a realizar ahora el mismo proceso pero con un conjunto de datos distinto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPty-attykwn"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "# Buscar kernel con mejor rendimiento\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "times = [-1] * len(kernels)\n",
        "acc = [-1] * len(kernels)\n",
        "f1 = [-1] * len(kernels)\n",
        "\n",
        "for i in range(len(kernels)):\n",
        "  print(kernels[i])\n",
        "  tic = timeit.default_timer()\n",
        "  svm_model = svm.SVC(kernel=kernels[i]).fit(mnist_X_train, mnist_y_train)\n",
        "  toc = timeit.default_timer()\n",
        "  times[i] = toc - tic\n",
        "\n",
        "  svm_pred = svm_model.predict(mnist_X_test)\n",
        "\n",
        "  acc[i] = metrics.accuracy_score(mnist_y_test, svm_pred)\n",
        "  f1[i] = metrics.f1_score(mnist_y_test, svm_pred, average='macro')\n",
        "\n",
        "print('\\nResultados')\n",
        "print(kernels)\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDjYuG6GymuD"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "# Buscar mejor valor de C para un kernel concreto\n",
        "Cs = [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
        "\n",
        "times = [-1] * len(Cs)\n",
        "acc = [-1] * len(Cs)\n",
        "f1 = [-1] * len(Cs)\n",
        "\n",
        "for i in range(len(Cs)):\n",
        "  print(Cs[i])\n",
        "  tic = timeit.default_timer()\n",
        "  svm_model = svm.SVC(kernel='rbf', C=Cs[i]).fit(mnist_X_train, mnist_y_train)\n",
        "  toc = timeit.default_timer()\n",
        "  times[i] = toc - tic\n",
        "\n",
        "  svm_pred = svm_model.predict(mnist_X_test)\n",
        "\n",
        "  acc[i] = metrics.accuracy_score(mnist_y_test, svm_pred)\n",
        "  f1[i] = metrics.f1_score(mnist_y_test, svm_pred, average='macro')\n",
        "\n",
        "print('\\nResultados')\n",
        "print(Cs)\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gw25SVMyoqR"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Número de árboles en el ensemble\n",
        "n = [10, 50, 100, 500, 1000]\n",
        "\n",
        "# Profundidad máxima de cada árbol\n",
        "max_depth_trees = [None, 3, 5]\n",
        "\n",
        "times = [None] * len(n)\n",
        "acc = [None] * len(n)\n",
        "f1 = [None] * len(n)\n",
        "\n",
        "\n",
        "for i in range(len(n)):\n",
        "  print(n[i])\n",
        "  times[i] = [-1] * len(max_depth_trees)\n",
        "  acc[i] = [-1] * len(max_depth_trees)\n",
        "  f1[i] = [-1] * len(max_depth_trees)\n",
        "  for j in range(len(max_depth_trees)):\n",
        "    print('  ' + str(max_depth_trees[j]))\n",
        "    tic = timeit.default_timer()\n",
        "    rf = RandomForestClassifier(n_estimators=n[i], max_depth=max_depth_trees[j]).fit(mnist_X_train, mnist_y_train)\n",
        "    toc = timeit.default_timer()\n",
        "    times[i][j] = toc - tic\n",
        "\n",
        "    rf_pred = rf.predict(mnist_X_test)\n",
        "\n",
        "    acc[i][j] = metrics.accuracy_score(mnist_y_test, rf_pred)\n",
        "    f1[i][j] = metrics.f1_score(mnist_y_test, rf_pred, average='macro')\n",
        "\n",
        "print('\\nResultados')\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he0cMM2fyql6"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import math\n",
        "\n",
        "n_attr = 784\n",
        "\n",
        "# Distintas configuraciones de capas ocultas\n",
        "# Cada tupla indica el número de neuronas en cada una de las capas ocultas\n",
        "hidden = [(10,), (50,), (50, 10,), (100,), (100, 50,), (round(math.sqrt(n_attr)),), (round(math.sqrt(n_attr)), round(math.log2(n_attr)))]\n",
        "times = [None] * len(hidden)\n",
        "acc = [None] * len(hidden)\n",
        "f1 = [None] * len(hidden)\n",
        "\n",
        "\n",
        "for i in range(len(hidden)):\n",
        "  print(hidden[i])\n",
        "  tic = timeit.default_timer()\n",
        "  mlp = MLPClassifier(hidden_layer_sizes=hidden[i], random_state=0).fit(mnist_X_train, mnist_y_train)\n",
        "  toc = timeit.default_timer()\n",
        "\n",
        "  times[i] = toc - tic\n",
        "\n",
        "  mlp_pred = mlp.predict(mnist_X_test)\n",
        "\n",
        "  acc[i] = metrics.accuracy_score(mnist_y_test, mlp_pred)\n",
        "  f1[i] = metrics.f1_score(mnist_y_test, mlp_pred, average='macro')\n",
        "\n",
        "print('\\nResultados')\n",
        "print(hidden)\n",
        "print(times)\n",
        "print(acc)\n",
        "print(f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7_BswaA2qLG"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Entrenar clasificadores clásicos con conjunto de datos de mnist\n",
        "tic = timeit.default_timer()\n",
        "lr = LogisticRegression(random_state=0).fit(mnist_X_train, mnist_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('LR train time: ' + str(toc - tic))\n",
        "\n",
        "lr_pred = lr.predict(mnist_X_test)\n",
        "\n",
        "print('LR acc: ' + str(metrics.accuracy_score(mnist_y_test, lr_pred)))\n",
        "print('LR F1: ' + str(metrics.f1_score(mnist_y_test, lr_pred, average='macro')))\n",
        "print('---')\n",
        "\n",
        "tic = timeit.default_timer()\n",
        "tree = tree.DecisionTreeClassifier().fit(mnist_X_train, mnist_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('Tree train time: ' + str(toc - tic))\n",
        "\n",
        "tree_pred = tree.predict(mnist_X_test)\n",
        "\n",
        "print('Tree acc: ' + str(metrics.accuracy_score(mnist_y_test, tree_pred)))\n",
        "print('Tree F1: ' + str(metrics.f1_score(mnist_y_test, tree_pred, average='macro')))\n",
        "print('---')\n",
        "\n",
        "tic = timeit.default_timer()\n",
        "gnb = GaussianNB().fit(mnist_X_train, mnist_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('GNB train time: ' + str(toc - tic))\n",
        "\n",
        "gnb_pred = gnb.predict(mnist_X_test)\n",
        "\n",
        "print('GNB acc: ' + str(metrics.accuracy_score(mnist_y_test, gnb_pred)))\n",
        "print('GNB F1: ' + str(metrics.f1_score(mnist_y_test, gnb_pred, average='macro')))\n",
        "print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy6P1q3r3xk0"
      },
      "outputs": [],
      "source": [
        "# Tener en cuenta que con ciertos conjuntos de datos puede ser MUY costoso obtener las predicciones en test\n",
        "tic = timeit.default_timer()\n",
        "knn = KNeighborsClassifier(n_neighbors=3).fit(mnist_X_train, mnist_y_train)\n",
        "toc = timeit.default_timer()\n",
        "print('kNN train time: ' + str(toc - tic))\n",
        "\n",
        "tic = timeit.default_timer()\n",
        "knn_pred = knn.predict(mnist_X_test)\n",
        "toc = timeit.default_timer()\n",
        "print('kNN testing time: ' + str(toc - tic))\n",
        "\n",
        "print('kNN acc: ' + str(metrics.accuracy_score(mnist_y_test, knn_pred)))\n",
        "print('kNN F1: ' + str(metrics.f1_score(mnist_y_test, knn_pred, average='macro')))\n",
        "print('---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRIEI2zK1Ipy"
      },
      "source": [
        "Analice y compare los resultados obtenidos con MNIST y responda a las mismas preguntas propuestas anteriormente. ¿Los resultados son similares? ¿Ha respondido lo mismo a todas las preguntas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv-Va4CM1WLV"
      },
      "source": [
        "Por último, copie y modifique el código necesario para utilizar el resto de conjuntos de datos propuestos al inicio del notebook.\n",
        "\n",
        "De nuevo, vuelva a responder a las mismas preguntas y analice si está respondiendo lo mismo o no en base al conjunto de datos utilizado.\n",
        "\n",
        "¿Son siempre mejor los métodos avanzados a los clásicos? ¿Son mejores los clásicos? ¿Depende del conjunto de datos?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MetodosPredictivos-NotebookSemana5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
